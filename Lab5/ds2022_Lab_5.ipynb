{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klPmYOG2mmXO"
      },
      "source": [
        "# Data Science Lab: Lab 5 - Anika Singh, Ravi Akalkotkar, Siddhant Singh\n",
        "**Due: Monday, March 7, 11:59 pm**\n",
        "\n",
        "Submit:\n",
        "1. A pdf of your notebook with solutions.\n",
        "2. A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
        "\n",
        "# Goals of this Lab\n",
        "\n",
        "1. Understanding Entropy\n",
        "2. Scraping data.\n",
        "3. Intro to Logistic Regression\n",
        "4. Revisiting CIFAR-10 and MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0iYJkbqm2lJ"
      },
      "source": [
        "## Problem 1 (Optional)\n",
        "\n",
        "Read Shannon's 1948 paper 'A Mathematical Theory of Communication'. Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication. \n",
        "\n",
        "https://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g-fBygena5C"
      },
      "source": [
        "## Problem 2: Scraping, Entropy and ICML papers\n",
        "\n",
        "ICML -- the International Conference on Machine Learning -- is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2021 papers from http://proceedings.mlr.press/v139/, and answer the following questions.\n",
        "\n",
        "1. What are the top 10 common words in the ICML papers?  \n",
        "2.  Let $Z$ be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of $Z$.\n",
        "3.  Synthesize a random paragraph using the marginal distribution over words. \n",
        "4. (Optional) Synthesize a random paragraph using an n-gram model on words. Synthesize a random paragraph using any model you want. We will learn more about generating text in later classes when we get into NLP. \n",
        "\n",
        "Note: downloading may take some time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F05Yb87PilFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934e0222-bed4-49e8-ac49-67dfb901267d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPdf2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n",
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.7/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from pdfminer) (3.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPdf2\n",
        "!pip install pdfminer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mI1qUTuJjJPp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import io\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics import log_loss\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import StringIO\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.high_level import extract_text\n",
        "from PyPDF2 import PdfFileReader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uxPPuGkUl4_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ccdcaf-1acf-4e39-84b7-5311825c5d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1183\n"
          ]
        }
      ],
      "source": [
        "url = \"http://proceedings.mlr.press/v139/\"\n",
        "\n",
        "read = requests.get(url)\n",
        "html_content = read.content\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "list_of_pdf = list()\n",
        "l = soup.find_all(\"div\", {\"class\": \"paper\"})\n",
        " \n",
        "count = 0\n",
        "# iterate through p for getting all the href links\n",
        "for link in l:\n",
        "    # accessed all the anchors tag from given p tag\n",
        "    p = link.find_all('a')\n",
        "    for a in p[1::3]:\n",
        "      count += 1\n",
        "      # converting the extension from .html to .pdf\n",
        "      pdf_link = (a.get('href')[:-4]) + \".pdf\"\n",
        "      \n",
        "      # added all the pdf links to set\n",
        "      list_of_pdf.append(pdf_link)\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NwtaJc7-qgtT"
      },
      "outputs": [],
      "source": [
        "for i in range(0,400): #scraping the first 400 (1/3rd of the entire data set), since Colab has insufficent RAM and I want to be able to complete the lab, so used smaller data set\n",
        "  response = requests.get(list_of_pdf[i])\n",
        "  text = extract_text(io.BytesIO(response.content))\n",
        "  f = open(\"myfile.txt\", \"a\")\n",
        "  f.write(text)\n",
        "  f.close()\n",
        "\n",
        "#f = open(\"myfile.txt\", \"r\")\n",
        "#print(f.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are the top 10 common words in the ICML papers?** (FIRST 400 PDFs ONLY aka 1/3rd of the entire data set)"
      ],
      "metadata": {
        "id": "wxAFx4H9fDn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"myfile.txt\", \"r\", encoding = \"utf-8\") as myfile:\n",
        "  textOne = myfile.readlines()\n",
        "\n",
        "vectorize = CountVectorizer()\n",
        "vectorize.fit(textOne)\n",
        "listOfWordsandOccurences = vectorize.vocabulary_\n",
        "\n",
        "with open('finalCountOfWords.txt', 'w', encoding = \"utf-8\") as fileOne:\n",
        "  print(vectorize.vocabulary_, file = fileOne)  \n",
        "\n",
        "NewVectorize = CountVectorizer().fit(textOne)\n",
        "words = NewVectorize.transform(textOne)\n",
        "\n",
        "wordNum = words.sum(axis = 0)\n",
        "\n",
        "frequencyOfWord = [(word, wordNum[0,idx]) for word, idx in NewVectorize.vocabulary_.items()]\n",
        "frequencyOfWord =sorted(frequencyOfWord, key = lambda x: x[1], reverse=True)\n",
        "top10 = frequencyOfWord[:10]\n",
        "\n",
        "print(\"Top 10 words in first 400 PDFs: \\n\", top10)\n",
        "\n"
      ],
      "metadata": {
        "id": "V6xTK9wgYABo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad73206b-7158-4020-b78a-a7634647a528"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words in first 400 PDFs: \n",
            " [('the', 190764), ('of', 97806), ('and', 91622), ('in', 74814), ('to', 64075), ('cid', 54129), ('for', 48315), ('is', 48040), ('we', 44840), ('that', 33330)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Let  Z  be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of  Z .**"
      ],
      "metadata": {
        "id": "lMs32ds-f0ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionaryProb = dict()\n",
        "for x,y in frequencyOfWord:\n",
        "  dictionaryProb.setdefault(a,[]).append(y)\n",
        "\n",
        "listOfProbabailites = list(dictionaryProb.values())\n",
        "#convert to pandas ds\n",
        "pd_series = pd.Series(listOfProbabailites)\n",
        "counts = pd_series.value_counts()\n",
        "entropyOne = entropy(counts)\n",
        "\n",
        "print(\"Entropy is: \", entropyOne)"
      ],
      "metadata": {
        "id": "FdzfYoL0oNK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110999b1-c7df-4d12-bec8-412615c2ddcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy is:  8.652721131567146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OZPy1D1pjt0"
      },
      "source": [
        "## Problem 3: Logistic Regression\n",
        "\n",
        "The following is a logistic regression problem using a real data set, made available by the authors of the book ``Applied Regression and Muiltilevel Modeling'' by Gelman and Hill. \n",
        "\n",
        "Download the data from the book, which you can find here http://www.stat.columbia.edu/~gelman/arm/software/. In particular, we are interested in the **arsenic** data set. The file **wells.dat** contains data on 3,020 households in Bangladesh. For each family, the natural arsenic level of each well was measured. In addition, the distance to the nearest safest well was measured. Each family is also described by a feature that relates to their community involvement, and a feature that gives the education level of the head of household. We are interested in building a model that predicts whether the family decided to switch wells or not, based on being informed of the level of arsenic in the well. Thus the \"label\" for this problem is the binary vector that is the first column of the dataset, labeled \"switch.\"\n",
        "\n",
        "1. Fit a logistic regression model using only an offset term and the distance to the nearest safe well.\n",
        "2. Plot your answer: that is, plot the probability of switching wells as a function of the distance to the nearest safe well. \n",
        "3. Interpreting logistic regression coefficients: Use the \"rule-of-4\" discussed in class on Thursday, to interpret the solution: what can you say about the change in the probability of switching wells, for every additional 100 meters of distance?\n",
        "4. Now solve a logistic regression incorporating the constant term, the distance and also arsenic levels. Report the coefficients\n",
        "5. Next we want to answer the question of which factor is more significant, distance, or arsenic levels? This is not a well specified question, since these two features have different units. One natural choice is to ask if after normalizing by the respective standard deviations of each feature, if moving one unit in one (normalized) feature predicts a larger change in probability of switching wells, than moving one unit in the other (also normalized) feature. Use this reasoning to answer the question. \n",
        "6. Now consider all the features in the data set. Also consider adding interaction terms among all features that have a large main effect. Use cross validation to build the best model you can (using your training set only), and then report the test error of your best model. (Note that since you have essentially unlimited access to your test set, this opens the door for massive overfitting. In contrast, Kaggle competitions try to mollify this by giving you only limited access to the test set.)\n",
        "7. (Optional) Now also play around with $\\ell_1$ and $\\ell_2$ regularization, and try to build the most accurate model you can (accuracy computed on the test data). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4eYv74Ue0HQ"
      },
      "outputs": [],
      "source": [
        "# convert dat file to pandas dataframe\n",
        "df = pd.read_csv('arsenic/wells.dat', sep = ' ', header = 0, index_col = 0)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKSTlpLJe0HR"
      },
      "source": [
        "1. Fit a logistic regression model using only an offset term and the distance to the nearest safe well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orTAPkX7e0HR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "X = np.column_stack((np.ones(df.shape[0]).T, df['dist']))\n",
        "y = df['switch']\n",
        "\n",
        "log = LogisticRegressionCV(Cs = [0.01, 0.1, 1, 10, 100], cv=5)\n",
        "log_fit = log.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujLYBsQwe0HS"
      },
      "source": [
        "2. Plot your answer: that is, plot the probability of switching wells as a function of the distance to the nearest safe well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgVr0pvFe0HS"
      },
      "outputs": [],
      "source": [
        "len_dist = max(df['dist'].values)\n",
        "dist_count = np.linspace(0, int(len_dist)-1, int(len_dist))\n",
        "\n",
        "distance_range = np.column_stack((np.ones(int(len_dist)), dist_count))\n",
        "\n",
        "prob_switching = log_fit.predict_proba(distance_range)\n",
        "\n",
        "plt.plot(dist_count, prob_switching[:, 0]) \n",
        "plt.xlabel(\"Distance to Nearest Safe Well\")\n",
        "plt.ylabel(\"Probability of Switching Wells Predictions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_YAXXc9e0HT"
      },
      "source": [
        "3. Interpreting logistic regression coefficients: Use the \"rule-of-4\" discussed in class on Thursday, to interpret the solution: what can you say about the change in the probability of switching wells, for every additional 100 meters of distance? **every additional 100m results in 15% increase chance of switching wells**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SJHefmze0HT"
      },
      "outputs": [],
      "source": [
        "print(log_fit.coef_)\n",
        "print(log_fit.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNJazcNPe0HU"
      },
      "source": [
        "4. Now solve a logistic regression incorporating the constant term, the distance and also arsenic levels. Report the coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJw5GnXye0HU"
      },
      "outputs": [],
      "source": [
        "X_normalize = np.column_stack((df['dist'], df['arsenic']))\n",
        "X_normalize = (X_normalize - np.mean(X_normalize))/np.std(X_normalize)\n",
        "\n",
        "print(X_normalize.shape, df.shape[0])\n",
        "\n",
        "X_2 = np.column_stack((np.ones(df.shape[0]).T, X_normalize))\n",
        "y_2 = df['switch']\n",
        "\n",
        "log_2 = LogisticRegressionCV(Cs = [0.01, 0.1, 1, 10, 100], cv=5)\n",
        "log_fit_2 = log_2.fit(X_2, y_2)\n",
        "\n",
        "print(log_fit_2.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru-Kpnete0HV"
      },
      "source": [
        "5. Next we want to answer the question of which factor is more significant, distance, or arsenic levels? This is not a well specified question, since these two features have different units. One natural choice is to ask if after normalizing by the respective standard deviations of each feature, if moving one unit in one (normalized) feature predicts a larger change in probability of switching wells, than moving one unit in the other (also normalized) feature. Use this reasoning to answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAepQ3MTe0HV"
      },
      "source": [
        "**According to the coefficients above, the most important feature is the arsenic levels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_g1VAzve0HW"
      },
      "source": [
        "6. Now consider all the features in the data set. Also consider adding interaction terms among all features that have a large main effect. Use cross validation to build the best model you can (using your training set only), and then report the test error of your best model. (Note that since you have essentially unlimited access to your test set, this opens the door for massive overfitting. In contrast, Kaggle competitions try to mollify this by giving you only limited access to the test set.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhZOorsre0HW"
      },
      "outputs": [],
      "source": [
        "X_3 = np.stack((np.ones(df.shape[0]).T, df['dist'], df['arsenic'], df['assoc'], df['educ'])).T\n",
        "\n",
        "fit_log_3 = LogisticRegressionCV(Cs=np.logspace(-3, 3, num=10), cv=10).fit(X_3, y)\n",
        "\n",
        "print(fit_log_3.score(X_3, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmzc_UK8p68j"
      },
      "source": [
        "## Problem 4: Logistic Regression and CIFAR-10\n",
        "\n",
        "In this problem you will explore the data set CIFAR-10, and you will use multinomial (multi-label) Logistic Regression to try to classify it. You will also explore visualizing the solution.\n",
        "\n",
        "1. (Optional) You can read about the CIFAR-10 and CIFAR-100 data sets here: https://www.cs.toronto.edu/~kriz/cifar.html.\n",
        "2. (Optional) OpenML curates a number of data sets. You will use a subset of CIFAR-10 provided by them. Read here for a description: https://www.openml.org/d/40926.\n",
        "3. Use the **fetch_openml** command from **sklearn.datasets** to import the CIFAR-10-Small data set. There are 20,000 data points. Do a train-test split on 3/4 - 1/4.\n",
        "4. Figure out how to display some of the images in this data set, and display a couple. While not high resolution, these should be recognizable if you are doing it correctly.\n",
        "5. You will run multi-class logistic regression on these using the cross entropy loss. You have to specify this specifically (**multi_class='multinomial'**). Use cross validation to see how good your accuracy can be. In this case, cross validate to find as good regularization coefficients as you can, for $\\ell_1$ and $\\ell_2$ regularization (i.e., for the Lasso and Ridge penalties), which are naturally supported in **sklearn.linear_model.LogisticRegression**. I recommend you use the solver **saga**. Note that this is quite a large problem: $20,000$ data points, each of them $3,072$-dimensional. Report your training and test losses.\n",
        "6. How sparse can you make your solutions without deteriorating your testing error too much? Here, I am asking you to try to obtain a sparse solution that has test accuracy that is close to the best solution you found. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE4TBHUMzwPc"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "cifar_small = fetch_openml('CIFAR_10_Small', version=1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G-gbC0Ce0HY"
      },
      "source": [
        "Use the fetch_openml command from sklearn.datasets to import the CIFAR-10-Small data set. There are 20,000 data points. Do a train-test split on 3/4 - 1/4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxsQ6KEXe0HY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = cifar_small['data'], cifar_small['target']\n",
        "print(X.shape)\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFiTvXAGe0HY"
      },
      "source": [
        "Figure out how to display some of the images in this data set, and display a couple. While not high resolution, these should be recognizable if you are doing it correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-uFNBJe1Z3e"
      },
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    img = X.iloc[i].to_numpy()\n",
        "    image = np.dstack((img[0:1024].reshape(32,32)/255.0,img[1024:2048].reshape(32,32)/255.0,img[2048:].reshape(32,32)/255.0))\n",
        "    plt.figure()\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSu1TGcN2rrT",
        "outputId": "63d9a9f8-940f-4ec4-ebfb-6b9d18a7b0c9"
      },
      "source": [
        "You will run multi-class logistic regression on these using the cross entropy loss. You have to specify this specifically (multi_class='multinomial'). Use cross validation to see how good your accuracy can be. In this case, cross validate to find as good regularization coefficients as you can, for  ‚Ñì1  and  ‚Ñì2  regularization (i.e., for the Lasso and Ridge penalties), which are naturally supported in sklearn.linear_model.LogisticRegression. I recommend you use the solver saga. Note that this is quite a large problem:  20,000  data points, each of them  3,072 -dimensional. Report your training and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV84xuhne0HZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "logistic = LogisticRegression(multi_class='multinomial')\n",
        "C = np.logspace(-2, 2, 5)\n",
        "\n",
        "cv = 5\n",
        "param_grid = [{'penalty': ['l1'], 'solver':['saga'], 'C': C}, {'penalty': ['l2'], 'solver':['saga'], 'C': C}]\n",
        "\n",
        "log_reg_search = GridSearchCV(logistic, param_grid, cv=cv)\n",
        "log_reg_search.fit(X_train, y_train)\n",
        "\n",
        "log_reg_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93vGNB5Ae0Ha"
      },
      "outputs": [],
      "source": [
        "best_est = log_reg_search.best_estimator_\n",
        "train_pred = best_est.predict_proba(X_train)\n",
        "print('training loss: ', log_loss(y_train, train_pred))\n",
        "\n",
        "test_pred = best_est.predict_proba(X_test)\n",
        "print('testing loss: ', log_loss(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSD1vJu1e0Ha"
      },
      "outputs": [],
      "source": [
        "print(log_reg_search.best_estimator_)\n",
        "score = log_reg_search.best_estimator_.score(X_test, y_test)\n",
        "print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5dlxpKe0Ha"
      },
      "source": [
        "How sparse can you make your solutions without deteriorating your testing error too much? Here, I am asking you to try to obtain a sparse solution that has test accuracy that is close to the best solution you found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH4q4tQhe0Hb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sparse_penalty = [0.1, 1, 10, 100]\n",
        "\n",
        "for i in sparse_penalty:\n",
        "    logistic = LogisticRegression(multi_class='multinomial', penalty = 'l1', solver = 'saga', C = i)\n",
        "    logistic.fit(X_train, y_train)\n",
        "    \n",
        "    train_pred = logistic.predict_proba(X_train)\n",
        "    print('C: ', i)\n",
        "    print('training loss: ', log_loss(y_train, train_pred))\n",
        "\n",
        "    test_pred = logistic.predict_proba(X_test)\n",
        "    print('testing loss: ', log_loss(y_test, test_pred))\n",
        "    \n",
        "    score = logistic.score(X_test, y_test)\n",
        "    print('Test Accuracy Score: ', score)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IPNeN-8e0Hb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sparse_penalty = [1000, 10000]\n",
        "\n",
        "for i in sparse_penalty:\n",
        "    logistic = LogisticRegression(multi_class='multinomial', penalty = 'l1', solver = 'saga', C = i)\n",
        "    logistic.fit(X_train, y_train)\n",
        "    \n",
        "    train_pred = logistic.predict_proba(X_train)\n",
        "    print('C: ', i)\n",
        "    print('training loss: ', log_loss(y_train, train_pred))\n",
        "\n",
        "    test_pred = logistic.predict_proba(X_test)\n",
        "    print('testing loss: ', log_loss(y_test, test_pred))\n",
        "    \n",
        "    score = logistic.score(X_test, y_test)\n",
        "    print('Test Accuracy Score: ', score)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PONK3tgBe0Hb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sparse_penalty = [0.001]\n",
        "\n",
        "for i in sparse_penalty:\n",
        "    logistic = LogisticRegression(multi_class='multinomial', penalty = 'l1', solver = 'saga', C = i)\n",
        "    logistic.fit(X_train, y_train)\n",
        "    \n",
        "    train_pred = logistic.predict_proba(X_train)\n",
        "    print('C: ', i)\n",
        "    print('training loss: ', log_loss(y_train, train_pred))\n",
        "\n",
        "    test_pred = logistic.predict_proba(X_test)\n",
        "    print('testing loss: ', log_loss(y_test, test_pred))\n",
        "    \n",
        "    score = logistic.score(X_test, y_test)\n",
        "    print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIHCI9CDe0Hc"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sparse_penalty = [0.0001]\n",
        "\n",
        "for i in sparse_penalty:\n",
        "    logistic = LogisticRegression(multi_class='multinomial', penalty = 'l1', solver = 'saga', C = i)\n",
        "    logistic.fit(X_train, y_train)\n",
        "    \n",
        "    train_pred = logistic.predict_proba(X_train)\n",
        "    print('C: ', i)\n",
        "    print('training loss: ', log_loss(y_train, train_pred))\n",
        "\n",
        "    test_pred = logistic.predict_proba(X_test)\n",
        "    print('testing loss: ', log_loss(y_test, test_pred))\n",
        "    \n",
        "    score = logistic.score(X_test, y_test)\n",
        "    print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2PoE8S3qnmn"
      },
      "source": [
        "## Problem 5: Multi-class Logistic Regression -- Visualizing the Solution\n",
        "\n",
        "You will repeat the previous problem but for the MNIST data set which you will find here: https://www.openml.org/d/554. As we have seen before, MNIST is a data set of handwritten digits, and is considered one of the \"easiest\" image recognition problems in computer vision. We will see here how well logistic regression does, as you did above on the CIFAR-10 subset. In addition, we will see that we can visualize the solution, and that in connection to this, sparsity can be useful.\n",
        "\n",
        "\n",
        "1. Use the **fetch_openml** command from to import the MNIST data set, and choose a reasonable train-test split (e.g., 75-25).\n",
        "2. Again run multi-class logistic regression on these using the cross entropy loss, as you did above. Try to optimize the hyperparameters. Report your training and test loss.\n",
        "3. Choose an $\\ell_1$ regularizer (penalty), and see if you can get a sparse solution with almost as good accuracy.\n",
        "4. Note that in Logistic Regression, the coefficients returned (i.e., the $\\beta$'s) are the same dimension as the data. Therefore we can pretend that the coefficients of the solution are an image of the same dimension, and plot it. Do this for the 10 sets of coefficients that correspond to the 10 classes. You should observe that, at least for the sparse solutions, these \"kind of\" look like the digits they are classifying. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfXYt9Wye0Hc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "X.shape\n",
        "X_train = X[:52500]\n",
        "y_train = y[:52500]\n",
        "X_test = X[52500:]\n",
        "y_test = y[52500:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBoWTn7CmdnU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# using the code from one of the lecture colabs to display a digit just to make sure it works\n",
        "index = 12\n",
        "img = X[index]\n",
        "label = y[index]\n",
        "img = img.reshape((28, 28))\n",
        "\n",
        "plt.title('The label is {label}'.format(label=label))\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yY7f_kQe0Hd"
      },
      "source": [
        "2. Again run multi-class logistic regression on these using the cross entropy loss, as you did above. Try to optimize the hyperparameters. Report your training and test loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4eS19V1e0Hd"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, solver='saga')\n",
        "C = np.logspace(-2, 2, 5)\n",
        "\n",
        "param_grid = [{'penalty': ['l1'], 'C': C}, {'penalty': ['l2'], 'C': C}]\n",
        "\n",
        "log_reg_search = GridSearchCV(logistic, param_grid)\n",
        "log_reg_search.fit(X_train, y_train)\n",
        "\n",
        "log_reg_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjEl2US4e0Hd"
      },
      "outputs": [],
      "source": [
        "\n",
        "logistic = LogisticRegression(multi_class='multinomial', penalty = 'l2', tol=0.1, solver = 'saga', C = 0.1)\n",
        "logistic.fit(X_train, y_train)\n",
        "\n",
        "train_pred = logistic.predict_proba(X_train)\n",
        "print('training loss: ', log_loss(y_train, train_pred))\n",
        "test_pred = logistic.predict_proba(X_test)\n",
        "print('testing loss: ', log_loss(y_test, test_pred))\n",
        "\n",
        "score = logistic.score(X_test, y_test)\n",
        "print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfaQC2pDe0Hd"
      },
      "outputs": [],
      "source": [
        "C = np.linspace(0.1, 10, num=100) #searched over this array earlier, took forever but got 6.7\n",
        "\n",
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, solver= 'saga', C=6.7, penalty='l2');\n",
        "logistic.fit(X_train, y_train)\n",
        "\n",
        "train_pred = logistic.predict_proba(X_train)\n",
        "print('training loss: ', log_loss(y_train, train_pred))\n",
        "test_pred = logistic.predict_proba(X_test)\n",
        "print('testing loss: ', log_loss(y_test, test_pred))\n",
        "\n",
        "score = logistic.score(X_test, y_test)\n",
        "print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-258R839e0He"
      },
      "source": [
        "3. Choose an  ‚Ñì1  regularizer (penalty), and see if you can get a sparse solution with almost as good accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAQ6lr24e0He"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, penalty = 'l1', solver = 'saga')\n",
        "C = np.logspace(-2, 2, 5)\n",
        "\n",
        "param_grid = [{'C': C}]\n",
        "\n",
        "log_reg_search = GridSearchCV(logistic, param_grid)\n",
        "log_reg_search.fit(X_train, y_train)\n",
        "\n",
        "log_reg_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcxL8fJle0He"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, penalty = 'l1', solver = 'saga', C=10)\n",
        "logistic.fit(X_train, y_train)\n",
        "\n",
        "train_pred = logistic.predict_proba(X_train)\n",
        "print('training loss: ', log_loss(y_train, train_pred))\n",
        "test_pred = logistic.predict_proba(X_test)\n",
        "print('testing loss: ', log_loss(y_test, test_pred))\n",
        "\n",
        "score = logistic.score(X_test, y_test)\n",
        "print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRtxREdLe0Hf"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, penalty = 'l1', solver = 'saga')\n",
        "C = np.logspace(-5, -1, 10)\n",
        "param_grid = [{'C': C}]\n",
        "log_reg_search = GridSearchCV(logistic, param_grid)\n",
        "log_reg_search.fit(X_train, y_train)\n",
        "log_reg_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9nPk78ge0Hf"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(multi_class='multinomial', tol = 0.1, penalty = 'l1', solver = 'saga', C=0.0359)\n",
        "logistic.fit(X_train, y_train)\n",
        "\n",
        "train_pred = logistic.predict_proba(X_train)\n",
        "print('training loss: ', log_loss(y_train, train_pred))\n",
        "test_pred = logistic.predict_proba(X_test)\n",
        "print('testing loss: ', log_loss(y_test, test_pred))\n",
        "\n",
        "score = logistic.score(X_test, y_test)\n",
        "print('Test Accuracy Score: ', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri1g5Ucue0Hf"
      },
      "source": [
        "4. Note that in Logistic Regression, the coefficients returned (i.e., the  ùõΩ 's) are the same dimension as the data. Therefore we can pretend that the coefficients of the solution are an image of the same dimension, and plot it. Do this for the 10 sets of coefficients that correspond to the 10 classes. You should observe that, at least for the sparse solutions, these \"kind of\" look like the digits they are classifying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr7BR2Yle0Hf"
      },
      "outputs": [],
      "source": [
        "\n",
        "for index in range(0, 10):\n",
        "    img = logistic.coef_[index]\n",
        "    img = img.reshape((28, 28))\n",
        "    plt.title('The label is {index}'.format(index=index))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of ds2022_Lab_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}